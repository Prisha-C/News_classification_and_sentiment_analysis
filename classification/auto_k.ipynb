{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "88a74dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Heading', 'Body', 'Category', 'URL'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fe74ad26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prish\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m290/290\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 143ms/step - accuracy: 0.3477 - loss: 1.8712 - val_accuracy: 0.6271 - val_loss: 1.1554\n",
      "Epoch 2/15\n",
      "\u001b[1m290/290\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 134ms/step - accuracy: 0.6238 - loss: 1.1493 - val_accuracy: 0.6478 - val_loss: 1.1152\n",
      "Epoch 3/15\n",
      "\u001b[1m290/290\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 135ms/step - accuracy: 0.6665 - loss: 1.0356 - val_accuracy: 0.6811 - val_loss: 0.9945\n",
      "Epoch 4/15\n",
      "\u001b[1m290/290\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 134ms/step - accuracy: 0.6818 - loss: 0.9950 - val_accuracy: 0.5904 - val_loss: 1.2229\n",
      "Epoch 5/15\n",
      "\u001b[1m290/290\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 135ms/step - accuracy: 0.6750 - loss: 1.0031 - val_accuracy: 0.7121 - val_loss: 0.9496\n",
      "Epoch 6/15\n",
      "\u001b[1m290/290\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 134ms/step - accuracy: 0.7060 - loss: 0.9102 - val_accuracy: 0.7000 - val_loss: 0.9577\n",
      "Epoch 7/15\n",
      "\u001b[1m290/290\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 137ms/step - accuracy: 0.7102 - loss: 0.8835 - val_accuracy: 0.6966 - val_loss: 0.9785\n",
      "Epoch 8/15\n",
      "\u001b[1m290/290\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 135ms/step - accuracy: 0.7257 - loss: 0.8485 - val_accuracy: 0.7100 - val_loss: 0.9380\n",
      "Epoch 9/15\n",
      "\u001b[1m290/290\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 136ms/step - accuracy: 0.7156 - loss: 0.8531 - val_accuracy: 0.7268 - val_loss: 0.8746\n",
      "Epoch 10/15\n",
      "\u001b[1m290/290\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 136ms/step - accuracy: 0.7411 - loss: 0.7891 - val_accuracy: 0.7108 - val_loss: 0.9052\n",
      "Epoch 11/15\n",
      "\u001b[1m290/290\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 135ms/step - accuracy: 0.7599 - loss: 0.7491 - val_accuracy: 0.7238 - val_loss: 0.8867\n",
      "Epoch 12/15\n",
      "\u001b[1m290/290\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 136ms/step - accuracy: 0.7661 - loss: 0.7207 - val_accuracy: 0.7363 - val_loss: 0.8677\n",
      "Epoch 13/15\n",
      "\u001b[1m290/290\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 135ms/step - accuracy: 0.7765 - loss: 0.6814 - val_accuracy: 0.7216 - val_loss: 0.8875\n",
      "Epoch 14/15\n",
      "\u001b[1m290/290\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 136ms/step - accuracy: 0.7800 - loss: 0.6523 - val_accuracy: 0.7238 - val_loss: 0.9200\n",
      "Epoch 15/15\n",
      "\u001b[1m290/290\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 136ms/step - accuracy: 0.7808 - loss: 0.6582 - val_accuracy: 0.7229 - val_loss: 0.9028\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 47ms/step - accuracy: 0.7539 - loss: 0.8123\n",
      "\n",
      "✅ Final Test Accuracy: 73.63%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional, Dropout\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# -------------------\n",
    "# Step 1: Load & Clean Data\n",
    "# -------------------\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"datasets/labelled.csv\")\n",
    "\n",
    "# Clean text\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text)\n",
    "    text = re.sub(r'\\@w+|\\#','', text)\n",
    "    text = re.sub(r'[^A-Za-z0-9 ]+', '', text)\n",
    "    return text\n",
    "\n",
    "df['Body'] = df['Body'].astype(str).apply(clean_text)\n",
    "texts = df['Body'].tolist()\n",
    "labels = df['Category'].tolist()\n",
    "\n",
    "# -------------------\n",
    "# Step 2: Tokenization\n",
    "# -------------------\n",
    "\n",
    "max_words = 8000\n",
    "max_len = 150\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(texts)\n",
    "X = tokenizer.texts_to_sequences(texts)\n",
    "X = pad_sequences(X, maxlen=max_len)\n",
    "\n",
    "# Label encoding\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(labels)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# -------------------\n",
    "# Step 3: Load GloVe Embeddings\n",
    "# -------------------\n",
    "\n",
    "embedding_dim = 100\n",
    "embedding_index = {}\n",
    "\n",
    "with open(\"glove.6B.100d.txt\", encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embedding_index[word] = coefs\n",
    "\n",
    "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if i < max_words:\n",
    "        embedding_vector = embedding_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "# -------------------\n",
    "# Step 4: Build the Model\n",
    "# -------------------\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(\n",
    "    input_dim=max_words,\n",
    "    output_dim=embedding_dim,\n",
    "    weights=[embedding_matrix],\n",
    "    input_length=max_len,\n",
    "    trainable=False\n",
    "))\n",
    "model.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Bidirectional(LSTM(32)))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(len(np.unique(y)), activation='softmax'))\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# -------------------\n",
    "# Step 5: Class Weights & Training\n",
    "# -------------------\n",
    "\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train\n",
    ")\n",
    "class_weights_dict = dict(enumerate(class_weights))\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=15,\n",
    "    batch_size=32,\n",
    "    class_weight=class_weights_dict,\n",
    "    callbacks=[early_stop]\n",
    ")\n",
    "\n",
    "# -------------------\n",
    "# Step 6: Evaluation\n",
    "# -------------------\n",
    "\n",
    "loss, acc = model.evaluate(X_test, y_test)\n",
    "print(f\"\\n✅ Final Test Accuracy: {acc:.2%}\")\n",
    "\n",
    "# -------------------\n",
    "# Step 7: Save Model (optional)\n",
    "# -------------------\n",
    "\n",
    "model.save(\"text_classification.keras\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26a552c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"text_classification_model.keras\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a950f87b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.save(\"text_classification_model.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c42cba39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_to_encoding(text):\n",
    "    return categories.get(text, -1)  # Return -1 if not found\n",
    "\n",
    "# Create the 'category_encoding' column\n",
    "df['category_encoding'] = df['Category'].apply(map_to_encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "42dbcb4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11583"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "09cc2dd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Heading</th>\n",
       "      <th>Body</th>\n",
       "      <th>Category</th>\n",
       "      <th>URL</th>\n",
       "      <th>category_encoding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>free speech not hate speech madras high court ...</td>\n",
       "      <td>madras high court issue significant remark ami...</td>\n",
       "      <td>Judiciary</td>\n",
       "      <td>https://www.indiatoday.in/law/high-courts/stor...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>comment take context say us cop mock indian st...</td>\n",
       "      <td>seattle police officer guild friday come defen...</td>\n",
       "      <td>Crime</td>\n",
       "      <td>https://www.indiatoday.in/world/story/indian-s...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>first meeting one nation one election committe...</td>\n",
       "      <td>first official meeting one nation one election...</td>\n",
       "      <td>Politics</td>\n",
       "      <td>https://www.indiatoday.in/india/story/one-nati...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>us airlines flight depressurize midair plummet...</td>\n",
       "      <td>united airlines jet head rome turn around less...</td>\n",
       "      <td>Crime</td>\n",
       "      <td>https://www.indiatoday.in/world/story/us-fligh...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>terrorist kill security force foil infiltratio...</td>\n",
       "      <td>three terrorist kill infiltration bid foil sec...</td>\n",
       "      <td>Crime</td>\n",
       "      <td>https://www.indiatoday.in/india/story/one-terr...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Heading  \\\n",
       "0  free speech not hate speech madras high court ...   \n",
       "1  comment take context say us cop mock indian st...   \n",
       "2  first meeting one nation one election committe...   \n",
       "3  us airlines flight depressurize midair plummet...   \n",
       "4  terrorist kill security force foil infiltratio...   \n",
       "\n",
       "                                                Body   Category  \\\n",
       "0  madras high court issue significant remark ami...  Judiciary   \n",
       "1  seattle police officer guild friday come defen...      Crime   \n",
       "2  first official meeting one nation one election...   Politics   \n",
       "3  united airlines jet head rome turn around less...      Crime   \n",
       "4  three terrorist kill infiltration bid foil sec...      Crime   \n",
       "\n",
       "                                                 URL  category_encoding  \n",
       "0  https://www.indiatoday.in/law/high-courts/stor...                  3  \n",
       "1  https://www.indiatoday.in/world/story/indian-s...                  4  \n",
       "2  https://www.indiatoday.in/india/story/one-nati...                  2  \n",
       "3  https://www.indiatoday.in/world/story/us-fligh...                  4  \n",
       "4  https://www.indiatoday.in/india/story/one-terr...                  4  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7f944a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "newsArticlesBody = df['Body'].to_list()\n",
    "\n",
    "categoryLabels = df['category_encoding'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "98190f65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 9266\n",
      "Validation set size: 1158\n",
      "Test set size: 1159\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training, validation, and test sets (80% train, 10% val, 10% test)\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    newsArticlesBody, categoryLabels, test_size=0.2, random_state=42, stratify = categoryLabels)\n",
    "\n",
    "val_texts, test_texts, val_labels, test_labels = train_test_split(\n",
    "    test_texts, test_labels, test_size=0.5, random_state=42,  stratify = test_labels)\n",
    "\n",
    "# Display the size of each dataset\n",
    "print(\"Train set size:\", len(train_texts))\n",
    "print(\"Validation set size:\", len(val_texts))\n",
    "print(\"Test set size:\", len(test_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0b6e0ffc",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "<class 'keras_hub.src.models.bert.bert_tokenizer.BertTokenizer'> could not be deserialized properly. Please ensure that components that are Python object instances (layers, models, etc.) returned by `get_config()` are explicitly deserialized in the model's `from_config()` method.\n\nconfig={'module': 'keras_hub.src.models.bert.bert_tokenizer', 'class_name': 'BertTokenizer', 'config': {'name': 'bert_tokenizer', 'trainable': True, 'dtype': {'module': 'keras', 'class_name': 'DTypePolicy', 'config': {'name': 'int32'}, 'registered_name': None}, 'config_file': 'tokenizer.json', 'vocabulary': None, 'sequence_length': None, 'lowercase': True, 'strip_accents': False, 'split': True, 'suffix_indicator': '##', 'oov_token': '[UNK]', 'special_tokens': None, 'special_tokens_in_strings': False}, 'registered_name': 'keras_hub>BertTokenizer'}.\n\nException encountered: Error when deserializing class 'BertTokenizer' using config={'name': 'bert_tokenizer', 'trainable': True, 'dtype': 'int32', 'config_file': 'tokenizer.json', 'vocabulary': None, 'sequence_length': None, 'lowercase': True, 'strip_accents': False, 'split': True, 'suffix_indicator': '##', 'oov_token': '[UNK]', 'special_tokens': None, 'special_tokens_in_strings': False}.\n\nException encountered: BertTokenizer requires `tensorflow` and `tensorflow-text` for text processing. Run `pip install tensorflow-text` to install both packages or visit https://www.tensorflow.org/install\n\nIf `tensorflow-text` is already installed, try importing it in a clean python session. Your installation may have errors.\n\nKerasHub uses `tf.data` and `tensorflow-text` to preprocess text on all Keras backends. If you are running on Jax or Torch, this installation does not need GPU support.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\ops\\operation.py:234\u001b[0m, in \u001b[0;36mOperation.from_config\u001b[1;34m(cls, config)\u001b[0m\n\u001b[0;32m    233\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig)\n\u001b[0;32m    235\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras_hub\\src\\models\\bert\\bert_tokenizer.py:76\u001b[0m, in \u001b[0;36mBertTokenizer.__init__\u001b[1;34m(self, vocabulary, lowercase, **kwargs)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_special_token(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[SEP]\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend_token\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 76\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m     77\u001b[0m     vocabulary\u001b[38;5;241m=\u001b[39mvocabulary,\n\u001b[0;32m     78\u001b[0m     lowercase\u001b[38;5;241m=\u001b[39mlowercase,\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m     80\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras_hub\\src\\tokenizers\\word_piece_tokenizer.py:342\u001b[0m, in \u001b[0;36mWordPieceTokenizer.__init__\u001b[1;34m(self, vocabulary, sequence_length, lowercase, strip_accents, split, split_on_cjk, suffix_indicator, oov_token, special_tokens, special_tokens_in_strings, dtype, **kwargs)\u001b[0m\n\u001b[0;32m    337\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    338\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput dtype must be an integer type or a string. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    339\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived: dtype=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    340\u001b[0m     )\n\u001b[1;32m--> 342\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(dtype\u001b[38;5;241m=\u001b[39mdtype, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m oov_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras_hub\\src\\tokenizers\\tokenizer.py:70\u001b[0m, in \u001b[0;36mTokenizer.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig_file \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig_file\u001b[39m\u001b[38;5;124m\"\u001b[39m, TOKENIZER_CONFIG_FILE)\n\u001b[1;32m---> 70\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_assets \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras_hub\\src\\layers\\preprocessing\\preprocessing_layer.py:10\u001b[0m, in \u001b[0;36mPreprocessingLayer.__init__\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m---> 10\u001b[0m     assert_tf_libs_installed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras_hub\\src\\utils\\tensor_utils.py:260\u001b[0m, in \u001b[0;36massert_tf_libs_installed\u001b[1;34m(symbol_name)\u001b[0m\n\u001b[0;32m    259\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tf_text \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m tf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 260\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m    261\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msymbol_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires `tensorflow` and `tensorflow-text` for \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    262\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext processing. Run `pip install tensorflow-text` to install \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    263\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mboth packages or visit https://www.tensorflow.org/install\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    264\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf `tensorflow-text` is already installed, try importing it \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    265\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min a clean python session. Your installation may have errors.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    266\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKerasHub uses `tf.data` and `tensorflow-text` to preprocess text \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    267\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon all Keras backends. If you are running on Jax or Torch, this \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    268\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstallation does not need GPU support.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    269\u001b[0m     )\n",
      "\u001b[1;31mImportError\u001b[0m: BertTokenizer requires `tensorflow` and `tensorflow-text` for text processing. Run `pip install tensorflow-text` to install both packages or visit https://www.tensorflow.org/install\n\nIf `tensorflow-text` is already installed, try importing it in a clean python session. Your installation may have errors.\n\nKerasHub uses `tf.data` and `tensorflow-text` to preprocess text on all Keras backends. If you are running on Jax or Torch, this installation does not need GPU support.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\saving\\serialization_lib.py:718\u001b[0m, in \u001b[0;36mdeserialize_keras_object\u001b[1;34m(config, custom_objects, safe_mode, **kwargs)\u001b[0m\n\u001b[0;32m    717\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 718\u001b[0m     instance \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mfrom_config(inner_config)\n\u001b[0;32m    719\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\ops\\operation.py:236\u001b[0m, in \u001b[0;36mOperation.from_config\u001b[1;34m(cls, config)\u001b[0m\n\u001b[0;32m    235\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 236\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    237\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError when deserializing class \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m using \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    238\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mException encountered: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    239\u001b[0m     )\n",
      "\u001b[1;31mTypeError\u001b[0m: Error when deserializing class 'BertTokenizer' using config={'name': 'bert_tokenizer', 'trainable': True, 'dtype': 'int32', 'config_file': 'tokenizer.json', 'vocabulary': None, 'sequence_length': None, 'lowercase': True, 'strip_accents': False, 'split': True, 'suffix_indicator': '##', 'oov_token': '[UNK]', 'special_tokens': None, 'special_tokens_in_strings': False}.\n\nException encountered: BertTokenizer requires `tensorflow` and `tensorflow-text` for text processing. Run `pip install tensorflow-text` to install both packages or visit https://www.tensorflow.org/install\n\nIf `tensorflow-text` is already installed, try importing it in a clean python session. Your installation may have errors.\n\nKerasHub uses `tf.data` and `tensorflow-text` to preprocess text on all Keras backends. If you are running on Jax or Torch, this installation does not need GPU support.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[79], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m y_val \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(val_labels)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m clf\u001b[38;5;241m.\u001b[39mfit(X_train, y_train, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, validation_data\u001b[38;5;241m=\u001b[39m(X_val, y_val))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\autokeras\\tasks\\text.py:163\u001b[0m, in \u001b[0;36mTextClassifier.fit\u001b[1;34m(self, x, y, epochs, callbacks, validation_split, validation_data, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\n\u001b[0;32m    107\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    108\u001b[0m     x\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    115\u001b[0m ):\n\u001b[0;32m    116\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search for the best model and hyperparameters for the AutoModel.\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \n\u001b[0;32m    118\u001b[0m \u001b[38;5;124;03m    It will search for the best model based on the performances on\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;124;03m            applicable).\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 163\u001b[0m     history \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfit(\n\u001b[0;32m    164\u001b[0m         x\u001b[38;5;241m=\u001b[39mx,\n\u001b[0;32m    165\u001b[0m         y\u001b[38;5;241m=\u001b[39my,\n\u001b[0;32m    166\u001b[0m         epochs\u001b[38;5;241m=\u001b[39mepochs,\n\u001b[0;32m    167\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[0;32m    168\u001b[0m         validation_split\u001b[38;5;241m=\u001b[39mvalidation_split,\n\u001b[0;32m    169\u001b[0m         validation_data\u001b[38;5;241m=\u001b[39mvalidation_data,\n\u001b[0;32m    170\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    171\u001b[0m     )\n\u001b[0;32m    172\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m history\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\autokeras\\auto_model.py:303\u001b[0m, in \u001b[0;36mAutoModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, callbacks, validation_split, validation_data, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    298\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validation_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m validation_split:\n\u001b[0;32m    299\u001b[0m     dataset, validation_data \u001b[38;5;241m=\u001b[39m data_utils\u001b[38;5;241m.\u001b[39msplit_dataset(\n\u001b[0;32m    300\u001b[0m         dataset, validation_split\n\u001b[0;32m    301\u001b[0m     )\n\u001b[1;32m--> 303\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtuner\u001b[38;5;241m.\u001b[39msearch(\n\u001b[0;32m    304\u001b[0m     x\u001b[38;5;241m=\u001b[39mdataset,\n\u001b[0;32m    305\u001b[0m     epochs\u001b[38;5;241m=\u001b[39mepochs,\n\u001b[0;32m    306\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[0;32m    307\u001b[0m     validation_data\u001b[38;5;241m=\u001b[39mvalidation_data,\n\u001b[0;32m    308\u001b[0m     validation_split\u001b[38;5;241m=\u001b[39mvalidation_split,\n\u001b[0;32m    309\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[0;32m    310\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    311\u001b[0m )\n\u001b[0;32m    313\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m history\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\autokeras\\engine\\tuner.py:200\u001b[0m, in \u001b[0;36mAutoTuner.search\u001b[1;34m(self, epochs, callbacks, validation_split, verbose, **fit_kwargs)\u001b[0m\n\u001b[0;32m    198\u001b[0m hp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moracle\u001b[38;5;241m.\u001b[39mget_space()\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_model_build(hp, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_kwargs)\n\u001b[1;32m--> 200\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_build(hp)\n\u001b[0;32m    201\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moracle\u001b[38;5;241m.\u001b[39mupdate_space(hp)\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39msearch(\n\u001b[0;32m    203\u001b[0m     epochs\u001b[38;5;241m=\u001b[39mepochs,\n\u001b[0;32m    204\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39mnew_callbacks,\n\u001b[0;32m    205\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_kwargs\n\u001b[0;32m    207\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras_tuner\\src\\engine\\tuner.py:164\u001b[0m, in \u001b[0;36mTuner._try_build\u001b[1;34m(self, hp)\u001b[0m\n\u001b[0;32m    161\u001b[0m keras\u001b[38;5;241m.\u001b[39mbackend\u001b[38;5;241m.\u001b[39mclear_session()\n\u001b[0;32m    162\u001b[0m gc\u001b[38;5;241m.\u001b[39mcollect()\n\u001b[1;32m--> 164\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_hypermodel(hp)\n\u001b[0;32m    165\u001b[0m \u001b[38;5;66;03m# Stop if `build()` does not return a valid model.\u001b[39;00m\n\u001b[0;32m    166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, keras\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mModel):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras_tuner\\src\\engine\\tuner.py:155\u001b[0m, in \u001b[0;36mTuner._build_hypermodel\u001b[1;34m(self, hp)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_build_hypermodel\u001b[39m(\u001b[38;5;28mself\u001b[39m, hp):\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m maybe_distribute(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribution_strategy):\n\u001b[1;32m--> 155\u001b[0m         model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhypermodel\u001b[38;5;241m.\u001b[39mbuild(hp)\n\u001b[0;32m    156\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_override_compile_args(model)\n\u001b[0;32m    157\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras_tuner\\src\\engine\\hypermodel.py:120\u001b[0m, in \u001b[0;36mHyperModel._build_wrapper\u001b[1;34m(self, hp, *args, **kwargs)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtunable:\n\u001b[0;32m    117\u001b[0m     \u001b[38;5;66;03m# Copy `HyperParameters` object so that new entries are not added\u001b[39;00m\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;66;03m# to the search space.\u001b[39;00m\n\u001b[0;32m    119\u001b[0m     hp \u001b[38;5;241m=\u001b[39m hp\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m--> 120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build(hp, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\autokeras\\graph.py:229\u001b[0m, in \u001b[0;36mGraph.build\u001b[1;34m(self, hp)\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n\u001b[0;32m    225\u001b[0m     temp_inputs \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    226\u001b[0m         keras_nodes[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_node_to_id[input_node]]\n\u001b[0;32m    227\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m input_node \u001b[38;5;129;01min\u001b[39;00m block\u001b[38;5;241m.\u001b[39minputs\n\u001b[0;32m    228\u001b[0m     ]\n\u001b[1;32m--> 229\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m block\u001b[38;5;241m.\u001b[39mbuild(hp, inputs\u001b[38;5;241m=\u001b[39mtemp_inputs)\n\u001b[0;32m    230\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m tree\u001b[38;5;241m.\u001b[39mflatten(outputs)\n\u001b[0;32m    231\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m output_node, real_output_node \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(block\u001b[38;5;241m.\u001b[39moutputs, outputs):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\autokeras\\engine\\block.py:38\u001b[0m, in \u001b[0;36mBlock._build_wrapper\u001b[1;34m(self, hp, *args, **kwargs)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_build_wrapper\u001b[39m(\u001b[38;5;28mself\u001b[39m, hp, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m hp\u001b[38;5;241m.\u001b[39mname_scope(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname):\n\u001b[1;32m---> 38\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m_build_wrapper(hp, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras_tuner\\src\\engine\\hypermodel.py:120\u001b[0m, in \u001b[0;36mHyperModel._build_wrapper\u001b[1;34m(self, hp, *args, **kwargs)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtunable:\n\u001b[0;32m    117\u001b[0m     \u001b[38;5;66;03m# Copy `HyperParameters` object so that new entries are not added\u001b[39;00m\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;66;03m# to the search space.\u001b[39;00m\n\u001b[0;32m    119\u001b[0m     hp \u001b[38;5;241m=\u001b[39m hp\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m--> 120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build(hp, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\autokeras\\blocks\\wrapper.py:125\u001b[0m, in \u001b[0;36mTextBlock.build\u001b[1;34m(self, hp, inputs)\u001b[0m\n\u001b[0;32m    123\u001b[0m input_node \u001b[38;5;241m=\u001b[39m tree\u001b[38;5;241m.\u001b[39mflatten(inputs)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    124\u001b[0m output_node \u001b[38;5;241m=\u001b[39m input_node\n\u001b[1;32m--> 125\u001b[0m output_node \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_block(hp, output_node)\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output_node\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\autokeras\\blocks\\wrapper.py:129\u001b[0m, in \u001b[0;36mTextBlock._build_block\u001b[1;34m(self, hp, output_node)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_build_block\u001b[39m(\u001b[38;5;28mself\u001b[39m, hp, output_node):\n\u001b[1;32m--> 129\u001b[0m     output_node \u001b[38;5;241m=\u001b[39m basic\u001b[38;5;241m.\u001b[39mBertBlock()\u001b[38;5;241m.\u001b[39mbuild(hp, output_node)\n\u001b[0;32m    130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output_node\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\autokeras\\engine\\block.py:38\u001b[0m, in \u001b[0;36mBlock._build_wrapper\u001b[1;34m(self, hp, *args, **kwargs)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_build_wrapper\u001b[39m(\u001b[38;5;28mself\u001b[39m, hp, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m hp\u001b[38;5;241m.\u001b[39mname_scope(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname):\n\u001b[1;32m---> 38\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m_build_wrapper(hp, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras_tuner\\src\\engine\\hypermodel.py:120\u001b[0m, in \u001b[0;36mHyperModel._build_wrapper\u001b[1;34m(self, hp, *args, **kwargs)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtunable:\n\u001b[0;32m    117\u001b[0m     \u001b[38;5;66;03m# Copy `HyperParameters` object so that new entries are not added\u001b[39;00m\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;66;03m# to the search space.\u001b[39;00m\n\u001b[0;32m    119\u001b[0m     hp \u001b[38;5;241m=\u001b[39m hp\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m--> 120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build(hp, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\autokeras\\blocks\\basic.py:643\u001b[0m, in \u001b[0;36mBertBlock.build\u001b[1;34m(self, hp, inputs)\u001b[0m\n\u001b[0;32m    640\u001b[0m input_tensor \u001b[38;5;241m=\u001b[39m tree\u001b[38;5;241m.\u001b[39mflatten(inputs)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    642\u001b[0m preset_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbert_base_en_uncased\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 643\u001b[0m tokenizer_layer \u001b[38;5;241m=\u001b[39m keras_nlp\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mBertPreprocessor\u001b[38;5;241m.\u001b[39mfrom_preset(\n\u001b[0;32m    644\u001b[0m     preset_name,\n\u001b[0;32m    645\u001b[0m     sequence_length\u001b[38;5;241m=\u001b[39mutils\u001b[38;5;241m.\u001b[39madd_to_hp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_sequence_length, hp),\n\u001b[0;32m    646\u001b[0m )\n\u001b[0;32m    647\u001b[0m bert_encoder \u001b[38;5;241m=\u001b[39m keras_nlp\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mBertBackbone\u001b[38;5;241m.\u001b[39mfrom_preset(preset_name)\n\u001b[0;32m    649\u001b[0m output_node \u001b[38;5;241m=\u001b[39m tokenizer_layer(ops\u001b[38;5;241m.\u001b[39mreshape(input_tensor, [\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras_hub\\src\\models\\preprocessor.py:186\u001b[0m, in \u001b[0;36mPreprocessor.from_preset\u001b[1;34m(cls, preset, config_file, **kwargs)\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mbackbone_cls \u001b[38;5;241m!=\u001b[39m backbone_cls:\n\u001b[0;32m    185\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m find_subclass(preset, \u001b[38;5;28mcls\u001b[39m, backbone_cls)\n\u001b[1;32m--> 186\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loader\u001b[38;5;241m.\u001b[39mload_preprocessor(\u001b[38;5;28mcls\u001b[39m, config_file, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras_hub\\src\\utils\\preset_utils.py:710\u001b[0m, in \u001b[0;36mKerasPresetLoader.load_preprocessor\u001b[1;34m(self, cls, config_file, **kwargs)\u001b[0m\n\u001b[0;32m    704\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_preprocessor\u001b[39m(\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mcls\u001b[39m, config_file\u001b[38;5;241m=\u001b[39mPREPROCESSOR_CONFIG_FILE, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    706\u001b[0m ):\n\u001b[0;32m    707\u001b[0m     \u001b[38;5;66;03m# If there is no `preprocessing.json` or it's for the wrong class,\u001b[39;00m\n\u001b[0;32m    708\u001b[0m     \u001b[38;5;66;03m# delegate to the super class loader.\u001b[39;00m\n\u001b[0;32m    709\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m check_file_exists(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreset, config_file):\n\u001b[1;32m--> 710\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mload_preprocessor(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    711\u001b[0m     preprocessor_json \u001b[38;5;241m=\u001b[39m load_json(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreset, config_file)\n\u001b[0;32m    712\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(check_config_class(preprocessor_json), \u001b[38;5;28mcls\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras_hub\\src\\utils\\preset_utils.py:638\u001b[0m, in \u001b[0;36mPresetLoader.load_preprocessor\u001b[1;34m(self, cls, config_file, **kwargs)\u001b[0m\n\u001b[0;32m    629\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_preprocessor\u001b[39m(\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mcls\u001b[39m, config_file\u001b[38;5;241m=\u001b[39mPREPROCESSOR_CONFIG_FILE, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    631\u001b[0m ):\n\u001b[0;32m    632\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load a prepocessor layer from the preset.\u001b[39;00m\n\u001b[0;32m    633\u001b[0m \n\u001b[0;32m    634\u001b[0m \u001b[38;5;124;03m    By default, we create a preprocessor from a tokenizer with default\u001b[39;00m\n\u001b[0;32m    635\u001b[0m \u001b[38;5;124;03m    arguments. This allow us to support transformers checkpoints by\u001b[39;00m\n\u001b[0;32m    636\u001b[0m \u001b[38;5;124;03m    only converting the backbone and tokenizer.\u001b[39;00m\n\u001b[0;32m    637\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 638\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_add_missing_kwargs(\u001b[38;5;28mself\u001b[39m, kwargs)\n\u001b[0;32m    639\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras_hub\\src\\models\\preprocessor.py:201\u001b[0m, in \u001b[0;36mPreprocessor._add_missing_kwargs\u001b[1;34m(cls, loader, kwargs)\u001b[0m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Fill in required kwargs when loading from preset.\u001b[39;00m\n\u001b[0;32m    191\u001b[0m \n\u001b[0;32m    192\u001b[0m \u001b[38;5;124;03mThis is a private method hit when loading a preprocessing layer that\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;124;03mencoders.\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer_cls:\n\u001b[1;32m--> 201\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m loader\u001b[38;5;241m.\u001b[39mload_tokenizer(\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer_cls)\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maudio_converter\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39maudio_converter_cls:\n\u001b[0;32m    203\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maudio_converter\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m loader\u001b[38;5;241m.\u001b[39mload_audio_converter(\n\u001b[0;32m    204\u001b[0m         \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39maudio_converter_cls\n\u001b[0;32m    205\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras_hub\\src\\utils\\preset_utils.py:655\u001b[0m, in \u001b[0;36mKerasPresetLoader.load_tokenizer\u001b[1;34m(self, cls, config_file, **kwargs)\u001b[0m\n\u001b[0;32m    653\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_tokenizer\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mcls\u001b[39m, config_file\u001b[38;5;241m=\u001b[39mTOKENIZER_CONFIG_FILE, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    654\u001b[0m     tokenizer_config \u001b[38;5;241m=\u001b[39m load_json(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreset, config_file)\n\u001b[1;32m--> 655\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_serialized_object(tokenizer_config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    656\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(tokenizer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mload_preset_assets\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    657\u001b[0m         tokenizer\u001b[38;5;241m.\u001b[39mload_preset_assets(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreset)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras_hub\\src\\utils\\preset_utils.py:727\u001b[0m, in \u001b[0;36mKerasPresetLoader._load_serialized_object\u001b[1;34m(self, config, **kwargs)\u001b[0m\n\u001b[0;32m    724\u001b[0m config \u001b[38;5;241m=\u001b[39m set_dtype_in_config(config, dtype)\n\u001b[0;32m    726\u001b[0m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs}\n\u001b[1;32m--> 727\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m keras\u001b[38;5;241m.\u001b[39msaving\u001b[38;5;241m.\u001b[39mdeserialize_keras_object(config)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\saving\\serialization_lib.py:720\u001b[0m, in \u001b[0;36mdeserialize_keras_object\u001b[1;34m(config, custom_objects, safe_mode, **kwargs)\u001b[0m\n\u001b[0;32m    718\u001b[0m     instance \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mfrom_config(inner_config)\n\u001b[0;32m    719\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 720\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    721\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m could not be deserialized properly. Please\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    722\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m ensure that components that are Python object\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    723\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m instances (layers, models, etc.) returned by\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    724\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `get_config()` are explicitly deserialized in the\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    725\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m model\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms `from_config()` method.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    726\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mconfig=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mException encountered: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    727\u001b[0m     )\n\u001b[0;32m    728\u001b[0m build_config \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbuild_config\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    729\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m build_config \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m instance\u001b[38;5;241m.\u001b[39mbuilt:\n",
      "\u001b[1;31mTypeError\u001b[0m: <class 'keras_hub.src.models.bert.bert_tokenizer.BertTokenizer'> could not be deserialized properly. Please ensure that components that are Python object instances (layers, models, etc.) returned by `get_config()` are explicitly deserialized in the model's `from_config()` method.\n\nconfig={'module': 'keras_hub.src.models.bert.bert_tokenizer', 'class_name': 'BertTokenizer', 'config': {'name': 'bert_tokenizer', 'trainable': True, 'dtype': {'module': 'keras', 'class_name': 'DTypePolicy', 'config': {'name': 'int32'}, 'registered_name': None}, 'config_file': 'tokenizer.json', 'vocabulary': None, 'sequence_length': None, 'lowercase': True, 'strip_accents': False, 'split': True, 'suffix_indicator': '##', 'oov_token': '[UNK]', 'special_tokens': None, 'special_tokens_in_strings': False}, 'registered_name': 'keras_hub>BertTokenizer'}.\n\nException encountered: Error when deserializing class 'BertTokenizer' using config={'name': 'bert_tokenizer', 'trainable': True, 'dtype': 'int32', 'config_file': 'tokenizer.json', 'vocabulary': None, 'sequence_length': None, 'lowercase': True, 'strip_accents': False, 'split': True, 'suffix_indicator': '##', 'oov_token': '[UNK]', 'special_tokens': None, 'special_tokens_in_strings': False}.\n\nException encountered: BertTokenizer requires `tensorflow` and `tensorflow-text` for text processing. Run `pip install tensorflow-text` to install both packages or visit https://www.tensorflow.org/install\n\nIf `tensorflow-text` is already installed, try importing it in a clean python session. Your installation may have errors.\n\nKerasHub uses `tf.data` and `tensorflow-text` to preprocess text on all Keras backends. If you are running on Jax or Torch, this installation does not need GPU support."
     ]
    }
   ],
   "source": [
    "clf = ak.TextClassifier(max_trials=2, metrics=['accuracy'])  # Adjust max_trials as needed\n",
    "# Convert text data to numpy arrays\n",
    "X_train = np.array(train_texts)\n",
    "y_train = np.array(train_labels)\n",
    "X_val = np.array(val_texts)\n",
    "y_val = np.array(val_labels)\n",
    "\n",
    "# Train the model\n",
    "clf.fit(X_train, y_train, epochs=10, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee0c03b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
